{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The code in this notebook, **when run in order**, will:\n",
    "\n",
    "#### (1) Produce a dataframe of tweets for a certain keyword (columns right now are, location and tweet text; can modify the search criteria as necessary (location lists, hashtags, etc.), saved as a csv\n",
    "\n",
    "#### (2) Delete the duplicates/very similar spam tweets but leave one copy behind. \n",
    "\n",
    "#### (3) Clean the remaining tweets and saves as a csv\n",
    "\n",
    "By the end of the notebook, you'll have a clean twitter dataframe that's ready for analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tweepy\n",
    "import pandas as pd\n",
    "import time\n",
    "import itertools\n",
    "import numpy as np\n",
    "from itertools import chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key= '3emnsuDpqLBXlxD5UPUxVnxTt'\n",
    "consumer_secret= 'ZPZBilbjbca7pYAmMv05hlNKsxLb4CJd6A5kNa2JNIs62BjF7m'\n",
    "access_token= '1406213154-M77vYKXz7wIAUzzJkqLVzfDH0D6K11xMJkzGHzJ'\n",
    "access_token_secret= 'iLcP4BuW4jWRYIHEiIZwsD5jgYlMY1aTmtSFwPQqbaQrX'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify tweet search/filtering criteria here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_words = \"mask free\"\n",
    "date_since = \"2021-03-10\"\n",
    "new_search = search_words + \" -filter:retweets\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set location labels and corresponding location coordinates and radii here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ** In the order of the counties listed in the google doc, and put in the Facebook chat\n",
    "\n",
    "locs_labels = ['Texas test',\n",
    "'New York County NY',\n",
    " 'County of San Francisco CA',\n",
    " 'Hudson County NJ',\n",
    " 'Suffolk County MA',\n",
    " 'Philadelphia County PA',\n",
    " 'Washington DC',\n",
    " 'Alexandria VA',\n",
    " 'Baltimore MD',\n",
    " 'Cook County IL',\n",
    " 'St. Louis County MO',\n",
    " 'Milwaukee County WI',\n",
    " 'Denver County CO',\n",
    " 'Orange County CA',\n",
    " 'Pinellas County FL',\n",
    " 'Ramsey County MN',\n",
    " 'Wayne County MI',\n",
    " 'Cuyahoga County OH',\n",
    " 'Dallas County TX',\n",
    " 'DeKalb County GA',\n",
    " 'Salt Lake County UT',\n",
    " 'Marion County IN',\n",
    " 'Jefferson County KY',\n",
    " 'Johnson County KS',\n",
    " 'King County WA']\n",
    "\n",
    "\n",
    "locs = ['29.88548725,-96.27736949,100mi',\n",
    "    '40.7741618671,-73.9697971209,100mi',\n",
    "    '37.7597150131,-122.693975573,100mi',\n",
    "    '40.7309049975,-74.0759554422,100mi',\n",
    "    '42.3486893925,-70.9856096697,100mi',\n",
    "    '40.0076360466,-75.1339446794,100mi', \n",
    "    '38.9047737146,-77.0163026702,100mi',\n",
    "    '38.8184535854,-77.0862060584,100mi',\n",
    "    '39.443072852,-76.6163229502,100mi',\n",
    "    '41.8954290221,-87.6461407931,100mi',\n",
    "    '38.6406885662,-90.4433975603,100mi',\n",
    "    '43.0154568883,-87.5805486607,100mi',\n",
    "    '39.7621646287,-104.875801423,100mi',\n",
    "    '33.6769134328,-117.776142822,100mi',\n",
    "    '27.9026762041,-82.7395539464,100mi',\n",
    "    '45.0170541432,-93.099606274,100mi',\n",
    "    '42.2847454761,-83.2611328686,100mi',\n",
    "    '41.6350590585,-81.7001959208,100mi',\n",
    "    '32.7666300195,-96.7778770663,100mi',\n",
    "    '33.7715468713,-84.226432416,100mi',\n",
    "    '40.6673139262,-111.923601307,100mi',\n",
    "    '39.7817096805,-86.1384716709,100mi',\n",
    "    '38.1871910002,-85.6591577058,100mi',\n",
    "    '38.8837685125,-94.8222610425,100mi',\n",
    "    '47.4909231926,-121.83595173,100mi']\n",
    "## Just zip locs_labels with locs and you'd have the coordinates mapped to their names\n",
    "\n",
    "mapped_locations = list(zip(locs_labels, locs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Texas test', '29.88548725,-96.27736949,100mi'),\n",
       " ('New York County NY', '40.7741618671,-73.9697971209,100mi'),\n",
       " ('County of San Francisco CA', '37.7597150131,-122.693975573,100mi'),\n",
       " ('Hudson County NJ', '40.7309049975,-74.0759554422,100mi'),\n",
       " ('Suffolk County MA', '42.3486893925,-70.9856096697,100mi'),\n",
       " ('Philadelphia County PA', '40.0076360466,-75.1339446794,100mi'),\n",
       " ('Washington DC', '38.9047737146,-77.0163026702,100mi'),\n",
       " ('Alexandria VA', '38.8184535854,-77.0862060584,100mi'),\n",
       " ('Baltimore MD', '39.443072852,-76.6163229502,100mi'),\n",
       " ('Cook County IL', '41.8954290221,-87.6461407931,100mi'),\n",
       " ('St. Louis County MO', '38.6406885662,-90.4433975603,100mi'),\n",
       " ('Milwaukee County WI', '43.0154568883,-87.5805486607,100mi'),\n",
       " ('Denver County CO', '39.7621646287,-104.875801423,100mi'),\n",
       " ('Orange County CA', '33.6769134328,-117.776142822,100mi'),\n",
       " ('Pinellas County FL', '27.9026762041,-82.7395539464,100mi'),\n",
       " ('Ramsey County MN', '45.0170541432,-93.099606274,100mi'),\n",
       " ('Wayne County MI', '42.2847454761,-83.2611328686,100mi'),\n",
       " ('Cuyahoga County OH', '41.6350590585,-81.7001959208,100mi'),\n",
       " ('Dallas County TX', '32.7666300195,-96.7778770663,100mi'),\n",
       " ('DeKalb County GA', '33.7715468713,-84.226432416,100mi'),\n",
       " ('Salt Lake County UT', '40.6673139262,-111.923601307,100mi'),\n",
       " ('Marion County IN', '39.7817096805,-86.1384716709,100mi'),\n",
       " ('Jefferson County KY', '38.1871910002,-85.6591577058,100mi'),\n",
       " ('Johnson County KS', '38.8837685125,-94.8222610425,100mi'),\n",
       " ('King County WA', '47.4909231926,-121.83595173,100mi')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapped_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(locs) # ensure that locs and loc_labels lens match; should have through zip funct anyway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(locs_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEXAS TEST ####\n",
    "\n",
    "\n",
    "#tweet_lst=[]\n",
    "#geoc=\"40.6546,73.5594,50mi\"\n",
    "#txgeoc = '29.88548725,-96.27736949,100mi'\n",
    "#migeoc = '25.6112362,-80.55170587,50mi'\n",
    "\n",
    "    \n",
    "\n",
    "# Creation of query method using parameters\n",
    "#try:\n",
    "    #tweets = tweepy.Cursor(api.search,q='mask free', lang=\"en\", since=date_since, geocode=txgeoc, tweet_mode='extended').items(1000) \n",
    "# Pulling information from tweets iterable object\n",
    "    #tweets_list = [[tweet.created_at, tweet.id, tweet.full_text] for tweet in tweets]\n",
    "    #tweets_sort = tweets_list.sort()\n",
    "    #no_dupes = list(k for k,_ in itertools.groupby(tweets_sort))\n",
    "    #print(tweets_list)\n",
    " # Creation of dataframe from tweets list\n",
    " # Add or remove columns as you remove tweet information\n",
    "    #tweets_d = pd.DataFrame(no_dupes)\n",
    "\n",
    "#except BaseException as e:\n",
    "    #print('failed on_status,',str(e))\n",
    "\n",
    "\n",
    "#tweets_f = pd.DataFrame(no_dupes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The code that does the actual scraping by location:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCRAPING TWEETS FOR ALL LOCATIONS # \n",
    "\n",
    "tweets_list = []\n",
    "for loc in mapped_locations:\n",
    "    try:\n",
    "        tweets = tweepy.Cursor(api.search,q=search_words, lang=\"en\", since=date_since, geocode=loc[1], tweet_mode='extended').items(2) \n",
    "        tweets_list.append((loc[0], [[tweet.full_text] for tweet in tweets]))\n",
    "        print(tweets_list)\n",
    "        tweets_df = pd.DataFrame(tweets_list)\n",
    "\n",
    "    except BaseException as e:\n",
    "        print('failed on_status,',str(e))\n",
    "        time.sleep(3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.to_csv('tweet_dataset.csv') # saves the dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet df loading and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tweepy\n",
    "import pandas as pd\n",
    "import time\n",
    "import itertools\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "import regex as re\n",
    "import unicodedata\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loads tweet dataframe, ensures datatypes are appropriate, flattens tweets into one list for ease, labels columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'tweet_dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-d7168f220e24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtweets_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tweet_dataset.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconverters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'1'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtweets_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tweets_list'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweets_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtweets_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'county'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'original_tweet'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tweets_list'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtweets_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    684\u001b[0m     )\n\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 946\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1179\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2008\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'tweet_dataset.csv'"
     ]
    }
   ],
   "source": [
    "tweets_df = pd.read_csv('tweet_dataset.csv', converters={'1': eval}, index_col=0)\n",
    "tweets_df['tweets_list'] = tweets_df['1'].apply(lambda x : np.array(x).flatten())\n",
    "tweets_df.columns = ['county', 'original_tweet', 'tweets_list']\n",
    "tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# texas  --- 226 total tweets\n",
    "#for index, tweet in enumerate(list(tweets_df['tweets_list'].iloc[0])):\n",
    "#    print(index, tweet)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, expands dataset so there is one row per tweet (before, all tweets for a county were in one row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_explode = tweets_df.explode('original_tweet')\n",
    "t_explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tex = t_explode.loc[t_explode['county'] == 'Texas test']\n",
    "\n",
    "# print(len(tex)) # confirm that this is 226; it is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***********\n",
    "# See note:\n",
    "\n",
    "I tried getting rid of duplicates a few ways; doing pairwise levenshtein distance comparisons between all the tweets took way too long, and creating a mask to then drop duplicate tweets was also complicated. I noticed that most very-similar-basically-duplicate tweets had the same text, but different handles and urls. So I took a slice of part of the text that was very likely to be shared between the similar tweets, created a new column in the dataset where each tweet-row got a corresponding tweet-slice, and then I deleted the rows where there were duplicate tweet-slices. As you can see, this resulted in a *lot* of data loss (3000+ tweets down to 850; see twitter_drop a few cells down), because many of them were duplicate/spam tweets.\n",
    "***********"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>county</th>\n",
       "      <th>original_tweet</th>\n",
       "      <th>tweets_list</th>\n",
       "      <th>tweet_str</th>\n",
       "      <th>tweet_slice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Texas test</td>\n",
       "      <td>[@Jim91674418 @fpatrickwelsh @tomselliott @chu...</td>\n",
       "      <td>[@Jim91674418 @fpatrickwelsh @tomselliott @chu...</td>\n",
       "      <td>[\"@Jim91674418 @fpatrickwelsh @tomselliott @ch...</td>\n",
       "      <td>odd Its just not helpful for the discussion or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Texas test</td>\n",
       "      <td>[@zulemashaden But shutting down business are ...</td>\n",
       "      <td>[@Jim91674418 @fpatrickwelsh @tomselliott @chu...</td>\n",
       "      <td>['@zulemashaden But shutting down business are...</td>\n",
       "      <td>y controlling the country like we are not free...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Texas test</td>\n",
       "      <td>[@SharpieDj @DianeDenizen @davidgadsby9 @tonyh...</td>\n",
       "      <td>[@Jim91674418 @fpatrickwelsh @tomselliott @chu...</td>\n",
       "      <td>[\"@SharpieDj @DianeDenizen @davidgadsby9 @tony...</td>\n",
       "      <td>ll3 Even when states lift the mask mandates th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Texas test</td>\n",
       "      <td>[What are some of the top compliance &amp;amp; eth...</td>\n",
       "      <td>[@Jim91674418 @fpatrickwelsh @tomselliott @chu...</td>\n",
       "      <td>['What are some of the top compliance &amp;amp; et...</td>\n",
       "      <td>stories from the past week? In the only weekl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Texas test</td>\n",
       "      <td>[@ITHERETWEETER1 @katyekellyeand1 @MySwilly @S...</td>\n",
       "      <td>[@Jim91674418 @fpatrickwelsh @tomselliott @chu...</td>\n",
       "      <td>['@ITHERETWEETER1 @katyekellyeand1 @MySwilly @...</td>\n",
       "      <td>KLORE1 @hellsoasis @DitseaYella @Promethiumban...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Denver County CO</td>\n",
       "      <td>[the absolute rage and disgust I feel right no...</td>\n",
       "      <td>[RT @subsix848: @fatherofyr @bergerbell It's c...</td>\n",
       "      <td>['the absolute rage and disgust I feel right n...</td>\n",
       "      <td>my zoology teacher was talking about going bac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Denver County CO</td>\n",
       "      <td>[RT @JBStonestreet: Mississippi third-grader L...</td>\n",
       "      <td>[RT @subsix848: @fatherofyr @bergerbell It's c...</td>\n",
       "      <td>['RT @JBStonestreet: Mississippi third-grader ...</td>\n",
       "      <td>a Booth has to wear a face mask to school. But...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Denver County CO</td>\n",
       "      <td>[@HackingDave They need to make like anti-zit ...</td>\n",
       "      <td>[RT @subsix848: @fatherofyr @bergerbell It's c...</td>\n",
       "      <td>['@HackingDave They need to make like anti-zit...</td>\n",
       "      <td>ks lol..but yes hopefully in a few months OTF ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Denver County CO</td>\n",
       "      <td>[RT @JBStonestreet: Mississippi third-grader L...</td>\n",
       "      <td>[RT @subsix848: @fatherofyr @bergerbell It's c...</td>\n",
       "      <td>['RT @JBStonestreet: Mississippi third-grader ...</td>\n",
       "      <td>a Booth has to wear a face mask to school. But...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Denver County CO</td>\n",
       "      <td>[Sparkx Radio KSPX Now Playing Faucet Failure ...</td>\n",
       "      <td>[RT @subsix848: @fatherofyr @bergerbell It's c...</td>\n",
       "      <td>['Sparkx Radio KSPX Now Playing Faucet Failure...</td>\n",
       "      <td>ean) - Ski Mask The Slump God https://t.co/amr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3768 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              county                                     original_tweet  \\\n",
       "0         Texas test  [@Jim91674418 @fpatrickwelsh @tomselliott @chu...   \n",
       "0         Texas test  [@zulemashaden But shutting down business are ...   \n",
       "0         Texas test  [@SharpieDj @DianeDenizen @davidgadsby9 @tonyh...   \n",
       "0         Texas test  [What are some of the top compliance &amp; eth...   \n",
       "0         Texas test  [@ITHERETWEETER1 @katyekellyeand1 @MySwilly @S...   \n",
       "..               ...                                                ...   \n",
       "12  Denver County CO  [the absolute rage and disgust I feel right no...   \n",
       "12  Denver County CO  [RT @JBStonestreet: Mississippi third-grader L...   \n",
       "12  Denver County CO  [@HackingDave They need to make like anti-zit ...   \n",
       "12  Denver County CO  [RT @JBStonestreet: Mississippi third-grader L...   \n",
       "12  Denver County CO  [Sparkx Radio KSPX Now Playing Faucet Failure ...   \n",
       "\n",
       "                                          tweets_list  \\\n",
       "0   [@Jim91674418 @fpatrickwelsh @tomselliott @chu...   \n",
       "0   [@Jim91674418 @fpatrickwelsh @tomselliott @chu...   \n",
       "0   [@Jim91674418 @fpatrickwelsh @tomselliott @chu...   \n",
       "0   [@Jim91674418 @fpatrickwelsh @tomselliott @chu...   \n",
       "0   [@Jim91674418 @fpatrickwelsh @tomselliott @chu...   \n",
       "..                                                ...   \n",
       "12  [RT @subsix848: @fatherofyr @bergerbell It's c...   \n",
       "12  [RT @subsix848: @fatherofyr @bergerbell It's c...   \n",
       "12  [RT @subsix848: @fatherofyr @bergerbell It's c...   \n",
       "12  [RT @subsix848: @fatherofyr @bergerbell It's c...   \n",
       "12  [RT @subsix848: @fatherofyr @bergerbell It's c...   \n",
       "\n",
       "                                            tweet_str  \\\n",
       "0   [\"@Jim91674418 @fpatrickwelsh @tomselliott @ch...   \n",
       "0   ['@zulemashaden But shutting down business are...   \n",
       "0   [\"@SharpieDj @DianeDenizen @davidgadsby9 @tony...   \n",
       "0   ['What are some of the top compliance &amp; et...   \n",
       "0   ['@ITHERETWEETER1 @katyekellyeand1 @MySwilly @...   \n",
       "..                                                ...   \n",
       "12  ['the absolute rage and disgust I feel right n...   \n",
       "12  ['RT @JBStonestreet: Mississippi third-grader ...   \n",
       "12  ['@HackingDave They need to make like anti-zit...   \n",
       "12  ['RT @JBStonestreet: Mississippi third-grader ...   \n",
       "12  ['Sparkx Radio KSPX Now Playing Faucet Failure...   \n",
       "\n",
       "                                          tweet_slice  \n",
       "0   odd Its just not helpful for the discussion or...  \n",
       "0   y controlling the country like we are not free...  \n",
       "0   ll3 Even when states lift the mask mandates th...  \n",
       "0    stories from the past week? In the only weekl...  \n",
       "0   KLORE1 @hellsoasis @DitseaYella @Promethiumban...  \n",
       "..                                                ...  \n",
       "12  my zoology teacher was talking about going bac...  \n",
       "12  a Booth has to wear a face mask to school. But...  \n",
       "12  ks lol..but yes hopefully in a few months OTF ...  \n",
       "12  a Booth has to wear a face mask to school. But...  \n",
       "12  ean) - Ski Mask The Slump God https://t.co/amr...  \n",
       "\n",
       "[3768 rows x 5 columns]"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting to string data type \n",
    "t_explode[\"tweet_str\"]= t_explode[\"original_tweet\"].astype(str) \n",
    "  \n",
    "# slicing till 2nd last element \n",
    "t_explode[\"tweet_slice\"] = t_explode[\"tweet_str\"].str.slice(50,1000) \n",
    "  \n",
    "# display \n",
    "t_explode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can look through slice list to ensure that the slices look right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_list = t_explode['tweet_slice'].tolist()\n",
    "\n",
    "\n",
    "#for i, v in enumerate(slice_list):\n",
    "#    print(i,v)\n",
    "#slice_list[184]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_drop = t_explode.drop_duplicates(subset=['tweet_slice'], keep='last')\n",
    "twitter_drop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleans the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = twitter_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower case\n",
    "df_clean['tweet_text'] = df_clean['tweet_str'].str.lower()\n",
    "\n",
    "# remove url links\n",
    "df_clean['tweet_text'] = df_clean['tweet_text'].apply(lambda x: re.sub(r'https?:\\/\\/\\S+', '', x))\n",
    "\n",
    "# remove url/website that didn't use http, is only checking for .com websites \n",
    "# so words that are seperated by a . are not removed\n",
    "df_clean['tweet_text'] = df_clean['tweet_text'].apply(lambda x: re.sub(r\"www\\.[a-z]?\\.?(com)+|[a-z]+\\.(com)\", '', x))\n",
    "# remove @mention\n",
    "df_clean['tweet_text'] = df_clean['tweet_text'].apply(lambda x: re.sub(r'@mention', '', x))\n",
    "# remove {link}\n",
    "df_clean['tweet_text'] = df_clean['tweet_text'].apply(lambda x: re.sub(r'{link}', '', x))\n",
    "# remove &text; html chars\n",
    "df_clean['tweet_text'] = df_clean['tweet_text'].apply(lambda x: re.sub(r'&[a-z]+;', '', x))\n",
    "# [video]\n",
    "df_clean['tweet_text'] = df_clean['tweet_text'].apply(lambda x: re.sub(r\"\\[video\\]\", '', x))\n",
    "# remove all remaining characters that aren't letters, white space, or \n",
    "# the following #:)(/\\='] that are used in emojis or hashtags\n",
    "df_clean['tweet_text'] = df_clean['tweet_text'].apply(lambda x: re.sub(r\"[^a-z\\s\\(\\-:\\)\\\\\\/\\];='#]\", '', x))\n",
    "df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do a spot check to make sure that the data has been cleaned well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_clean['tweet_text'].iloc[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twitter_drop['tweet_str'].iloc[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.to_csv('cleaned_tweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#########################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
